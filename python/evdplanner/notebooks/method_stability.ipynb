{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Method Stability\n",
    "\n",
    "In this notebook we evaluate 2 modes of stability of our method.\n",
    "\n",
    "1. Given that our neural network to find facial landmarks has a mean error of around 4mm, how much impact does this have on the location of Kocher's Point?\n",
    "2. Given that Kocher's point moves around a bit, based on the landmarks, how much does this affect the location of the Target Point?\n",
    "\n",
    "We assess this by calculating the location of KP and TP using the manually placed landmarks, and then moving the landmarks around a bit to see how much the location of KP and TP changes.\n",
    "Based on earlier results, we know that the predicted landmarks have a mean error of 4mm, so we will go a small step further and move the landmarks around a bit more than that, to see how much the location of KP and TP changes."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from pathlib import Path\n",
    "from random import choice\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd\n",
    "import seaborn as sns\n",
    "from scipy.stats import spearmanr\n",
    "\n",
    "from evdplanner.cli import set_verbosity\n",
    "from evdplanner.generation import measure_kocher\n",
    "from evdplanner.geometry import Mesh\n",
    "from evdplanner.linalg import Vec3\n",
    "from evdplanner.markups import MarkupManager\n",
    "from evdplanner.rendering import find_target, Ray, IntersectionSort\n",
    "\n",
    "skin_mesh_file = \"mesh_skin.stl\"\n",
    "ventricles_mesh_file = \"mesh_ventricles.stl\"\n",
    "\n",
    "manual_landmarks_file = \"landmarks_skin.mrk.json\"\n",
    "\n",
    "evd_file = \"EVD.mrk.json\"\n",
    "\n",
    "samples_dir = Path(r\"S:\\E_ResearchData\\evdplanner\\Samples\")\n",
    "test_dir = Path(r\"S:\\E_ResearchData\\evdplanner\\Test\")\n",
    "\n",
    "scores_file = Path(r\"S:\\E_ResearchData\\evdplanner\\MajorityVoting.csv\")\n",
    "\n",
    "set_verbosity(0)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll use all patients in the dataset for this evaluation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "patients = [x.resolve() for x in test_dir.iterdir() if x.is_dir()]\n",
    "patients += [x.resolve() for x in samples_dir.iterdir() if x.is_dir()]\n",
    "print(f\"Found {len(patients)} patients\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In the paper we performed 1000 randomizations for statistical significance.\n",
    "Now that we've got a bit more time after the resubmission, we can also check with more runs."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "records = []\n",
    "\n",
    "check_radially = True\n",
    "radius = 1.5\n",
    "objective_distance_weight = 0.75\n",
    "thickness_threshold = 10.0\n",
    "depth_threshold = 80.0\n",
    "\n",
    "n_tests = 5000  # 1000 for the resubmission\n",
    "wiggles_per_patient = 1  # 10 for the resubmission\n",
    "n_patients = len(patients)\n",
    "max_error = 10.0\n",
    "\n",
    "while len(records) < n_tests:\n",
    "    patient = choice(patients)\n",
    "    print(f\"Processing patient {patient.name} ({len(records) + 1}/{n_tests})...\")\n",
    "\n",
    "    try:\n",
    "        skin_mesh = Mesh.load(str(patient / skin_mesh_file))\n",
    "        ventricles_mesh = Mesh.load(str(patient / ventricles_mesh_file))\n",
    "        gt_landmarks = MarkupManager.load(patient / manual_landmarks_file)\n",
    "    except:\n",
    "        print(f\"Skin Mesh or landmarks not found for patient {patient.name}.\")\n",
    "        continue\n",
    "\n",
    "    gt_nasion = Vec3(*gt_landmarks.find_fiducial(\"Nasion\").position)\n",
    "    gt_left_ear = Vec3(*gt_landmarks.find_fiducial(\"Pre-Auricle Left\").position)\n",
    "    gt_right_ear = Vec3(*gt_landmarks.find_fiducial(\"Pre-Auricle Right\").position)\n",
    "\n",
    "    print(\"Measuring GT Kocher's points...\")\n",
    "    gt_left_kp, gt_right_kp = measure_kocher(\n",
    "        mesh=skin_mesh,\n",
    "        nasion=gt_nasion,\n",
    "        left_ear=gt_left_ear,\n",
    "        right_ear=gt_right_ear,\n",
    "    )\n",
    "\n",
    "    print(\"Finding GT target points...\")\n",
    "    gt_left_tp, _ = find_target(\n",
    "        ventricles_mesh,\n",
    "        gt_left_kp,\n",
    "        check_radially=check_radially,\n",
    "        radius=radius,\n",
    "        objective_distance_weight=objective_distance_weight,\n",
    "        thickness_threshold=thickness_threshold,\n",
    "        depth_threshold=depth_threshold,\n",
    "    )\n",
    "    gt_right_tp, _ = find_target(\n",
    "        ventricles_mesh,\n",
    "        gt_right_kp,\n",
    "        check_radially=check_radially,\n",
    "        radius=radius,\n",
    "        objective_distance_weight=objective_distance_weight,\n",
    "        thickness_threshold=thickness_threshold,\n",
    "        depth_threshold=depth_threshold,\n",
    "    )\n",
    "\n",
    "    for wiggle in range(wiggles_per_patient):\n",
    "        print(f\"Wiggling {wiggle + 1}/{wiggles_per_patient}...\")\n",
    "\n",
    "        left_record = {\n",
    "            \"Patient\": patient.name,\n",
    "            \"Side\": \"Left\",\n",
    "        }\n",
    "        right_record = {\n",
    "            \"Patient\": patient.name,\n",
    "            \"Side\": \"Right\",\n",
    "        }\n",
    "\n",
    "        wiggle_nasion = gt_nasion + Vec3(\n",
    "            np.random.uniform(-max_error, max_error),\n",
    "            np.random.uniform(-max_error, max_error),\n",
    "            np.random.uniform(-max_error, max_error),\n",
    "        )\n",
    "        wiggle_left_ear = gt_left_ear + Vec3(\n",
    "            np.random.uniform(-max_error, max_error),\n",
    "            np.random.uniform(-max_error, max_error),\n",
    "            np.random.uniform(-max_error, max_error),\n",
    "        )\n",
    "        wiggle_right_ear = gt_right_ear + Vec3(\n",
    "            np.random.uniform(-max_error, max_error),\n",
    "            np.random.uniform(-max_error, max_error),\n",
    "            np.random.uniform(-max_error, max_error),\n",
    "        )\n",
    "\n",
    "        # Project the wiggled points to the skin mesh\n",
    "        wiggle_nasion = skin_mesh.intersect(\n",
    "            ray=Ray(skin_mesh.origin, (wiggle_nasion - skin_mesh.origin).unit_vector),\n",
    "            sorting=IntersectionSort.Farthest,\n",
    "        )\n",
    "        wiggle_left_ear = skin_mesh.intersect(\n",
    "            ray=Ray(skin_mesh.origin, (wiggle_left_ear - skin_mesh.origin).unit_vector),\n",
    "            sorting=IntersectionSort.Farthest,\n",
    "        )\n",
    "        wiggle_right_ear = skin_mesh.intersect(\n",
    "            ray=Ray(skin_mesh.origin, (wiggle_right_ear - skin_mesh.origin).unit_vector),\n",
    "            sorting=IntersectionSort.Farthest,\n",
    "        )\n",
    "\n",
    "        if wiggle_nasion is None or wiggle_left_ear is None or wiggle_right_ear is None:\n",
    "            print(f\"Intersection failed for patient {patient.name}.\")\n",
    "            continue\n",
    "\n",
    "        wiggle_nasion = wiggle_nasion.position\n",
    "        wiggle_left_ear = wiggle_left_ear.position\n",
    "        wiggle_right_ear = wiggle_right_ear.position\n",
    "\n",
    "        left_record[\"N Error\"] = (wiggle_nasion - gt_nasion).length\n",
    "        right_record[\"N Error\"] = (wiggle_nasion - gt_nasion).length\n",
    "        left_record[\"LPA Error\"] = (wiggle_left_ear - gt_left_ear).length\n",
    "        right_record[\"LPA Error\"] = (wiggle_left_ear - gt_left_ear).length\n",
    "        left_record[\"RPA Error\"] = (wiggle_right_ear - gt_right_ear).length\n",
    "        right_record[\"RPA Error\"] = (wiggle_right_ear - gt_right_ear).length\n",
    "\n",
    "        for lm in [\"N Error\", \"LPA Error\", \"RPA Error\"]:\n",
    "            print(f\"Left  {lm} = {left_record[lm]} mm\")\n",
    "            print(f\"right {lm} = {right_record[lm]} mm\")\n",
    "\n",
    "        print(\"Measuring predicted Kocher's points...\")\n",
    "        # Measure Kocher's points using the wiggled landmarks\n",
    "        predicted_left_kp, predicted_right_kp = measure_kocher(\n",
    "            mesh=skin_mesh,\n",
    "            nasion=wiggle_nasion,\n",
    "            left_ear=wiggle_left_ear,\n",
    "            right_ear=wiggle_right_ear,\n",
    "        )\n",
    "\n",
    "        left_record[r\"$K$ Difference\"] = (predicted_left_kp - gt_left_kp).length\n",
    "        right_record[r\"$K$ Difference\"] = (predicted_right_kp - gt_right_kp).length\n",
    "\n",
    "        print(f\"Left  K Difference = {left_record[r'$K$ Difference']} mm\")\n",
    "        print(f\"Right K Difference = {right_record[r'$K$ Difference']} mm\")\n",
    "\n",
    "        print(\"Finding target points...\")\n",
    "        # Find target points using the predicted Kocher's points\n",
    "        predicted_left_tp, _ = find_target(\n",
    "            ventricles_mesh,\n",
    "            predicted_left_kp,\n",
    "            check_radially=check_radially,\n",
    "            radius=radius,\n",
    "            objective_distance_weight=objective_distance_weight,\n",
    "            thickness_threshold=thickness_threshold,\n",
    "            depth_threshold=depth_threshold,\n",
    "        )\n",
    "        predicted_right_tp, _ = find_target(\n",
    "            ventricles_mesh,\n",
    "            predicted_right_kp,\n",
    "            check_radially=check_radially,\n",
    "            radius=radius,\n",
    "            objective_distance_weight=objective_distance_weight,\n",
    "            thickness_threshold=thickness_threshold,\n",
    "            depth_threshold=depth_threshold,\n",
    "        )\n",
    "\n",
    "        left_record[r\"$T$ Difference\"] = (predicted_left_tp - gt_left_tp).length\n",
    "        right_record[r\"$T$ Difference\"] = (predicted_right_tp - gt_right_tp).length\n",
    "        left_record[\"Distance\"] = (predicted_left_tp - predicted_left_kp).length\n",
    "        right_record[\"Distance\"] = (predicted_right_tp - predicted_right_kp).length\n",
    "\n",
    "        print(f\"Left  T Difference = {left_record[r'$T$ Difference']} mm\")\n",
    "        print(f\"Right T Difference = {right_record[r'$T$ Difference']} mm\")\n",
    "\n",
    "        records.extend([left_record, right_record])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = pd.DataFrame.from_records(records)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "scores = pd.read_csv(scores_file)\n",
    "scores = scores.rename(columns={\"PatientID\": \"Patient\", \"Score\": \"Kakarla\"})\n",
    "df = df.merge(scores, on=[\"Patient\", \"Side\"], how=\"left\")\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "p = sns.pairplot(\n",
    "    data=df,\n",
    "    hue=\"Side\",\n",
    "    markers=[\"o\", \"X\"],\n",
    "    diag_kind=None,\n",
    "    x_vars=[\n",
    "        \"N Error\",\n",
    "        \"LPA Error\",\n",
    "        \"RPA Error\",\n",
    "    ],\n",
    "    y_vars=[\n",
    "        r\"$K$ Difference\",\n",
    "    ],\n",
    ")\n",
    "\n",
    "p.figure.suptitle(r\"$K$ deviation versus landmark error\", y=1.1)\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "pairs = [\n",
    "    (\"N Error\", r\"$K$ Difference\"),\n",
    "    (\"LPA Error\", r\"$K$ Difference\"),\n",
    "    (\"RPA Error\", r\"$K$ Difference\"),\n",
    "    (r\"$K$ Difference\", r\"$T$ Difference\"),\n",
    "]\n",
    "\n",
    "fig, axs = plt.subplots(\n",
    "    nrows=1,\n",
    "    ncols=len(pairs),\n",
    "    figsize=(16, 6),\n",
    "    constrained_layout=True,\n",
    ")\n",
    "\n",
    "for ax, (x, y) in zip(axs, pairs, strict=False):\n",
    "    # Calculate Spearman correlation coefficient\n",
    "    res = spearmanr(df[x], df[y])\n",
    "    corr = res.statistic\n",
    "    p = res.pvalue\n",
    "    print(f\"Spearman correlation between {x} and {y}: {corr:.2f} (p-value: {p:.2e})\")\n",
    "\n",
    "    p = sns.scatterplot(\n",
    "        data=df,\n",
    "        x=x,\n",
    "        y=y,\n",
    "        hue=\"Kakarla\",\n",
    "        style=\"Side\",\n",
    "        ax=ax,\n",
    "    )\n",
    "    p.set_title(f\"{y} vs {x}\")\n",
    "    p.set_xlabel(x)\n",
    "    p.set_ylabel(y)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "selected_columns = [\"N Error\", \"LPA Error\", \"RPA Error\", r\"$K$ Difference\", r\"$T$ Difference\"]\n",
    "\n",
    "correlations = df[selected_columns].corr(method=\"spearman\")\n",
    "p_values = df[selected_columns].corr(method=lambda x, y: spearmanr(x, y).pvalue)\n",
    "mask = np.triu(np.ones_like(correlations, dtype=bool))\n",
    "\n",
    "sns.set_style(\"white\")\n",
    "sns.set_context(\"notebook\", font_scale=1.2)\n",
    "\n",
    "fig, axs = plt.subplots(1, 3, figsize=(16, 6))\n",
    "sns.heatmap(\n",
    "    correlations,\n",
    "    # mask=mask,\n",
    "    annot=False,\n",
    "    fmt=\".2f\",\n",
    "    cmap=\"coolwarm\",\n",
    "    cbar_kws={\"label\": \"Spearman correlation coefficient\"},\n",
    "    vmin=-1,\n",
    "    vmax=1,\n",
    "    center=0,\n",
    "    ax=axs[0],\n",
    ")\n",
    "\n",
    "for i in range(correlations.shape[0]):\n",
    "    for j in range(correlations.shape[1]):\n",
    "        # if i >= j:\n",
    "        #     continue\n",
    "\n",
    "        corr = f\"{correlations.iloc[i, j]:.2f}\"\n",
    "        if correlations.iloc[i, j] > 0:\n",
    "            corr = f\"+{corr}\"\n",
    "        else:\n",
    "            corr = f\"{corr}\"\n",
    "\n",
    "        if p_values.iloc[i, j] < 0.001:\n",
    "            p_val = r\"$^{*}$\"\n",
    "        elif p_values.iloc[i, j] < 0.01:\n",
    "            p_val = r\"$^{**}$\"\n",
    "        elif p_values.iloc[i, j] < 0.05:\n",
    "            p_val = r\"$^{***}$\"\n",
    "        else:\n",
    "            p_val = \"\"\n",
    "\n",
    "        color = \"black\" if abs(correlations.iloc[i, j]) < 0.5 else \"white\"\n",
    "\n",
    "        axs[0].text(\n",
    "            i + 0.5,\n",
    "            j + 0.5,\n",
    "            f\"{corr}{p_val}\",\n",
    "            ha=\"center\",\n",
    "            va=\"center\",\n",
    "            color=color,\n",
    "            fontsize=12,\n",
    "        )\n",
    "\n",
    "axs[0].set_title(\"Correlation Matrix\")\n",
    "\n",
    "sns.boxenplot(\n",
    "    data=df,\n",
    "    x=\"Side\",\n",
    "    y=r\"$K$ Difference\",\n",
    "    hue=\"Side\",\n",
    "    ax=axs[1],\n",
    "    order=[\"Left\", \"Right\"],\n",
    ")\n",
    "axs[1].set_title(r\"$K$ Difference by Side\")\n",
    "axs[1].set_xlabel(\"\")\n",
    "axs[1].set_ylabel(r\"Difference (mm)\")\n",
    "\n",
    "sns.boxenplot(\n",
    "    data=df,\n",
    "    x=\"Side\",\n",
    "    y=r\"$T$ Difference\",\n",
    "    hue=\"Side\",\n",
    "    ax=axs[2],\n",
    "    order=[\"Left\", \"Right\"],\n",
    ")\n",
    "axs[2].set_title(r\"$T$ Difference by Side\")\n",
    "axs[2].set_xlabel(\"\")\n",
    "axs[2].set_ylabel(r\"Difference (mm)\")\n",
    "\n",
    "plt.tight_layout()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# mean and std of the errors\n",
    "for col in selected_columns:\n",
    "    mean = df[col].mean()\n",
    "    std = df[col].std()\n",
    "    median = df[col].median()\n",
    "    low_ci = df[col].quantile(0.025)\n",
    "    high_ci = df[col].quantile(0.975)\n",
    "    print(\n",
    "        f\"{col}: {mean:.2f} Â± {std:.2f} (median: {median:.2f}, 95% CI: [{low_ci:.2f}, {high_ci:.2f}])\"\n",
    "    )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df.to_csv(Path(r\"./stability_results.csv\"), index=False)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": ".venv",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.12.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
